input {
  kafka {
    bootstrap_servers => "localhost:9092"
    topics => [ "JMETER_METRICS" ]
    codec => "json"
    group_id => "GRP_JMETER_METRICS_LOGSTASH_01"
    auto_offset_reset => "latest"
  }
}

filter {
  json {
    source => "message"
  }

  # Logstash has strange implementation for json filter above.
  # If it sees a field named @timestamp, then it expects it to be a seconds based value (UNIX rather than UNIX_MS) and tries to parse on its own to reuse as event's @timestamp.
  # In our case, @timestamp is milli-seconds based long, hence it fails to parse and causes a warning with _timestampparsefailure tag.

  if "_timestampparsefailure" in [tags] {
  date {
    match => [ "_@timestamp", "UNIX_MS" ]
    remove_tag => "_timestampparsefailure"
    remove_field => "_@timestamp"
  }
}

date{
  match => [ "ElapsedTime", "ISO8601", "yyyy-MM-dd HH:mm:ss" ]
  target => ElapsedTime
}

date{
  match => [ "Timestamp", "ISO8601" ]
  target => Timestamp
}

date{
  match => [ "SampleStartTime", "ISO8601" ]
  target => SampleStartTime
}

date{
  match => [ "SampleEndTime", "ISO8601" ]
  target => SampleEndTime
}

ruby {
  code => "if (event.get('tags') != nil) && event.get('tags').length == 0 then event.remove('tags') end"
}
}

output {
  #stdout { codec => rubydebug }

  elasticsearch {
    index => "jmeter_metrics-%{+yyyy-MM-dd}"
    hosts => ["localhost:9201"]
    user => "elastic"
    password => "changeme"
    document_type => "logs"
  }
}
